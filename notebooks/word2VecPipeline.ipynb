{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnscrapercondaa354d8fb32614818906b42d96b0e4fbd",
   "display_name": "Python 3.8.5 64-bit ('NNScraper': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# custom made functions (can be a pain to import sometimes)\n",
    "import sys\n",
    "sys.path.insert(0,os.path.abspath('../src/helper'))\n",
    "from customPandas import *\n",
    "#saving the model\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "source": [
    "## Loading the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39125532/file-does-not-exist-in-jupyter-notebook\n",
    "dataPath = os.path.abspath('../data')\n",
    "fileName = 'ramen-ratings.csv'\n",
    "df = pd.read_csv(f'{dataPath}/{fileName}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Review #           Brand  \\\n",
       "0      2580       New Touch   \n",
       "1      2579        Just Way   \n",
       "2      2578          Nissin   \n",
       "3      2577         Wei Lih   \n",
       "4      2576  Ching's Secret   \n",
       "\n",
       "                                             Variety Style Country Top Ten  \n",
       "0                          T's Restaurant Tantanmen    Cup   Japan     NaN  \n",
       "1  Noodles Spicy Hot Sesame Spicy Hot Sesame Guan...  Pack  Taiwan     NaN  \n",
       "2                      Cup Noodles Chicken Vegetable   Cup     USA     NaN  \n",
       "3                      GGE Ramen Snack Tomato Flavor  Pack  Taiwan     NaN  \n",
       "4                                    Singapore Curry  Pack   India     NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review #</th>\n      <th>Brand</th>\n      <th>Variety</th>\n      <th>Style</th>\n      <th>Country</th>\n      <th>Top Ten</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2580</td>\n      <td>New Touch</td>\n      <td>T's Restaurant Tantanmen</td>\n      <td>Cup</td>\n      <td>Japan</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2579</td>\n      <td>Just Way</td>\n      <td>Noodles Spicy Hot Sesame Spicy Hot Sesame Guan...</td>\n      <td>Pack</td>\n      <td>Taiwan</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2578</td>\n      <td>Nissin</td>\n      <td>Cup Noodles Chicken Vegetable</td>\n      <td>Cup</td>\n      <td>USA</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2577</td>\n      <td>Wei Lih</td>\n      <td>GGE Ramen Snack Tomato Flavor</td>\n      <td>Pack</td>\n      <td>Taiwan</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2576</td>\n      <td>Ching's Secret</td>\n      <td>Singapore Curry</td>\n      <td>Pack</td>\n      <td>India</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## Separating the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Series([], Name: Stars, dtype: float64)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def cleanStars(value):\n",
    "    if value == 'Unrated':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "df.Stars = df.Stars.apply(cleanStars)\n",
    "# changing an incorrect dtype\n",
    "df = df.astype({'Stars':'float64'}) \n",
    "df[df.Stars == 'Unrated'].Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Stars']\n",
    "df = df.drop(['Stars'],axis=1)"
   ]
  },
  {
   "source": [
    "## Learning more about custom pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review # = 0, Top ten = 5\n",
    "def dropUselessFeatures(col):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanStyle = ColumnTransformer(transformers=[ \n",
    "('dropUselessFeatures', FunctionTransformer(dropUselessFeatures, validate=False),[1,2,3,4])], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can view the effect of your custome pipeline using fit_transform\n",
    "df2 = pd.DataFrame(cleanStyle.fit_transform(df))\n",
    "totalPercentageNullData(df2)"
   ]
  },
  {
   "source": [
    "## Word2Vec NN creation\n",
    "[source](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['GGE', 'Ramen', 'Snack', 'Tomato', 'Flavor'],\n",
       " ['Singapore', 'Curry'],\n",
       " ['Kimchi', 'song', 'Song', 'Ramen']]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "tokenized_corpus[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1567"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "#to convert back from indices to words\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, \"T's\"),\n",
       " (1, 'Restaurant'),\n",
       " (2, 'Tantanmen'),\n",
       " (3, 'Noodles'),\n",
       " (4, 'Spicy'),\n",
       " (5, 'Hot'),\n",
       " (6, 'Sesame'),\n",
       " (7, 'Guan-miao'),\n",
       " (8, 'Cup'),\n",
       " (9, 'Chicken')]"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "from itertools import islice\n",
    "# https://stackoverflow.com/questions/7971618/return-first-n-keyvalue-pairs-from-dict\n",
    "# I just want to see the first n items in a dict\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "n_items = take(10, idx2word.items())\n",
    "n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 2],\n",
       "       [1, 0],\n",
       "       [1, 2],\n",
       "       [2, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [3, 5],\n",
       "       [4, 3],\n",
       "       [4, 5]])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "idx_pairs[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"T's\",\n",
       " 'Restaurant',\n",
       " \"T's\",\n",
       " 'Tantanmen',\n",
       " 'Restaurant',\n",
       " \"T's\",\n",
       " 'Restaurant',\n",
       " 'Tantanmen',\n",
       " 'Tantanmen',\n",
       " \"T's\"]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "[idx2word[i] for item in idx_pairs for i in item][:10]"
   ]
  },
  {
   "source": [
    "## Neural Network\n",
    "[source](https://gist.github.com/mbednarski/c24c683fa7d4f2148fc5fdfc51246b91)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable, profiler\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    word_vec_one_hot = np.zeros(vocabulary_size)\n",
    "    word_vec_one_hot[word2idx[word]] = 1\n",
    "    return word_vec_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 10\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        indices = [word2idx[w] for w in words]\n",
    "        for i in range(len(indices)):\n",
    "            # center word, context\n",
    "            # i is center word index\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_idx = i + w\n",
    "                if context_idx < 0 or context_idx >= len(indices) or i == context_idx:\n",
    "                    continue\n",
    "                center_vec_one_hot = np.zeros(vocabulary_size)\n",
    "                center_vec_one_hot[indices[i]] = 1\n",
    "                \n",
    "                context_idx = indices[context_idx]\n",
    "                yield center_vec_one_hot, context_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 501/501 [1:18:11<00:00,  9.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Network definition\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "\n",
    "\n",
    "for epo in tqdm(range(501)):\n",
    "    avg_loss = 0\n",
    "    samples = 0\n",
    "    for data, target in train_generator():\n",
    "        x = Variable(torch.from_numpy(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        samples += len(y_true)\n",
    "        \n",
    "        a1 = torch.matmul(W1, x)\n",
    "        a2 = torch.matmul(W2, a1)\n",
    "\n",
    "        log_softmax = F.log_softmax(a2, dim=0)\n",
    "\n",
    "        network_pred_dist = F.softmax(log_softmax, dim=0)\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        W1.data -= 0.01 * W1.grad.data\n",
    "        W2.data -= 0.01 * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "    # if epo % 50 == 0:\n",
    "    #     print(avg_loss / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 1567])"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/henri/Documents/Post Lighthouse-Lab work/KaggleML/ramen-ratings/src/models'"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "modelPath = os.path.abspath('../src/models')\n",
    "modelPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scikitplot.decomposition import plot_pca_2d_projection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(W1.data.numpy().T)\n",
    "# proj = pca.transform(W1.data.numpy().T)\n",
    "# ax = plot_pca_2d_projection(pca, W1.data.numpy().T, np.array(vocabulary), figsize=(12,12),feature_labels=None, text_fontsize=12) #feature_labels=vocabulary,\n",
    "# ax.legend(None)\n",
    "# for i, txt in enumerate(words):\n",
    "#     ax.annotate(txt, (proj[i,0], proj[i,1]), size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(W1, f'{modelPath}/W1.sav')\n",
    "torch.save(W2, f'{modelPath}/W2.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "#the_model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector_v(word):\n",
    "    return W1[:, word2idx[word]].data.numpy()\n",
    "\n",
    "def get_word_vector_u(word):\n",
    "    return W2[word2idx[word],:].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland to Warsaw is like Germany to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "distances = [(v, cosine(yyy, 1 * get_word_vector_u(v) + 1 * get_word_vector_v(v))) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(\"T's\", 1.0273612160235643),\n",
       " ('Restaurant', 0.8840634450316429),\n",
       " ('Tantanmen', 0.9146503508090973),\n",
       " ('Noodles', 0.605650007724762),\n",
       " ('Spicy', 0.6084803342819214)]"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "distances[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The random ingredient choosen is Artificially\nchicken: 0.47\nspice: 0.08\nmushroom: 0.05\ndelight: 0.13\n"
     ]
    }
   ],
   "source": [
    "randomIngredient = random.choice(vocabulary)\n",
    "context_to_predict = get_word_vector_v(randomIngredient)\n",
    "hidden = Variable(torch.from_numpy(context_to_predict)).float()\n",
    "a = torch.matmul(W2, hidden)\n",
    "probs = F.softmax(a, dim=0).data.numpy()\n",
    "print(f'The random ingredient choosen is {randomIngredient}')\n",
    "for context, prob in zip(words, probs):\n",
    "    if prob >.05:\n",
    "        print(f'{context}: {prob:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}